\chapter{Implementation}

  In order to analyze how effective our attacks can steal links from the training graph of an inductive trained graph neural network, we performed several experiments by attacking GNNs that have been trained to perform node classification.
  In this chapter we want to go through the implementation of our attacks.
  We covered multiple datasets and types of graph neural network models, leading to an amount of \textasciitilde200 experiments.
  Due computational and time constraints, most of the parameters to optimize the attacks remain unexplored.

  \section{Datasets}

    For all our experiments, we used 3 datasets in total.
    In the table below, they are listed with their most interesting attributes.
    All of the datasets we used are from the same domain - Citation Networks.
    This is because these networks are open source and similar to social networks in their structure.
    Due to computational constraints we didn't use social network datasets like reddit.

    \vspace{0.48cm}
    \begin{table}[!h]
      \centering
      \footnotesize
      \begin{tabular}{l|l|l|l|l}
        \toprule
        Name & Number of Nodes & Number of Edges & Number of Classes & Feature Amount \\
        \midrule
        Cora & 2708            & 5429            & 7                 & 1433 \\
        CiteSeer & 3327        & 4732            & 6                 & 3703 \\
        Pubmed & 19717         & 44338           & 3                 & 500 \\
        \bottomrule
      \end{tabular}
      \caption{Dataset Information}
      \label{table:datasets}
    \end{table}

    \subsection*{Sample Datasets for Experiments}

      In total we train target models on all three datasets - \emph{Cora}, \emph{CiteSeer} and \emph{Pubmed} .
      To train the attacker models, we sample an adversary dataset based on the incomplete subgraph.

      \subsubsection*{Same Dataset Distributions - Attack 1-2}
        Let $D_{f_t} = (V_{f_t}, E_{f_t})$ be one of our three original datasets with $|V_{f_t}|$ nodes and $|E_{f_t}|$ edges.
        We denote $D' = (V',E')$ as subgraph of $D_{f_t}$.
        $D_{f_t}$ is used to train our target model $f_t$, while $D'$ is used to sample $G_{A}$, which is a graph, that was modified by deleting some known edges to simulate an incomplete graph.
        $D_A$ is obtained by querying $f_t$ on a subgraph of $G_A$. 

        \vspace{0.48cm}
        \begin{figure}[h!]
          \begin{center}
            \begin{tikzpicture}
              
              % Definitions
              \tikzstyle{Dataset} = [rectangle, rounded corners,minimum width=2cm, minimum height=1cm, text centered, draw=black]
              \tikzstyle{arrow} = [thick,->,>=stealth]

              \node (Dft) [Dataset] {$D_{f_t}$};
              \node (D') [Dataset, right of=Dft, xshift=3cm] {$D'$};
              \node (GA) [Dataset, right of=D', xshift=2.5cm] {$G_A$};
              \node (DA) [Dataset, above right of=GA, xshift=2.5cm] {$D_A$};
              \node (Gs) [Dataset, below right of=GA, xshift=2.5cm] {$G_s$};

              \draw [arrow] (Dft) -- node[anchor=south] {subgraph} (D');
              \draw [arrow] (D') -- node[anchor=south] {modify} (GA);
              \draw [arrow] (GA) -- node[anchor=south] {} (Gs);
              \draw [arrow] (GA) -- node[anchor=south] {} (DA);

              

            \end{tikzpicture}
          \end{center}
          \caption{Sampling Datasets - Same Dataset Distribution - Attack 1-2}
          \label{figure:sample-datasets-attack1-2}
        \end{figure}

        We define $G_A = (V_A, E_A)$ with $V_A = V'$ and $E_A = E'$, which is now an exact copy of $D'$.
        To sample $D_A$, we first collect a set of positive samples $pos = \{(i,j, 1)$ | $\forall i,j \in V': (i,j) \in E' \wedge |pos| < ((1 - \alpha) * |E'|))\}$, containing pairs of nodes, that are connected in $D'$, where $\alpha$ denotes the percentage of known edges.
        Now, we collect a set of negative samples $neg = \{(i,j, 0)$ | $\forall i,j \in V': (i,j) \not\in E' \wedge |neg| < ((1 - \alpha) * |E'|))\}$, containing pairs of nodes, that are not connected in $D'$.
        We then delete all edges we sampled for $pos$, in our graph clone $G_A$, to simulate the missing edges, we want to steal.
        This leads to $E_A = \{(i,j)$ | $\forall (i,j) \in E': (i,j) \not\in pos\}$.
        $G_A$ is now a modified graph that contains less edges then the original graph $D'$ and we define a raw-dataset $raw = pos \cup neg$, containing the positive and negative samples obtained from $D'$.
        As the next step, we create the adversary's dataset $D_A = \{(post_{ij}, l)$ | $\forall (i,j,l)\in raw: post_{ij} = concat(f_t(G_A, i), f_t(G_A, j))\}$ for \emph{Attack 1} and $D_A = \{(dist_{ij}, l)$ | $\forall (i,j,l)\in raw: dist_{ij} = dist(post_i, post_j)\}$ for \emph{Attack 2}.
        $f_t(G_A, i)$ returns the node classification output posterior of the target model, when it is queried on $i$ given the adversary's graph $G_A$.
        $concat(a, b)$ concatenates the output posteriors $a$ and $b$ with each other returning the feature we will train the attacker model on.
        $l$ denotes the label either being 1 (positive sample) or 0 (negative sample).
        $dist(a,b) = [Cosine(a,b), ..., Sqeuclidean(a,b)]$, return the vector containing 8 different distance values like described in Section \refeq{section:threat-model}.
        With our adversary's dataset $D_A$ we can now continue training our attacker model using either $post_{ij}$ or $dist_{ij}$ as input features and $l$ as class.
        $G_s$ represents the incomplete subgraph of $D_{f_t}$, which will be used to test the adversary performance on $f_f$.
        More precisely, how well does the adversary can predict correctly whether a link between two nodes in $G_s$ is missing or not.

      \subsubsection*{Different Dataset Distributions - Attack 3}

        Let $D_{f_A}$ and $D_{f_t}$ be two of our three original datasets.
        We use $D_{f_A}$ to train the shadow model $f_A$ and $D_{f_t}$ to train our target model $f_t$.
        We denote $D_1'$ as subgraph of $D_{f_A}$ and $D_2'$ as subgraph of $D_{f_t}$.
        To sample $D_A$ we follow the same steps as described in \emph{Attack 1}.
        Firstly we create an incomplete graph $G_A$ by randomly deleting some edges.
        We then sample $D_A$ by querying $f_A$ on $G_A$.
        To obtain $G_s$ we modify $D_2'$ by randomly deleting some edges and use it to test the adversary performance on $f_t$.

        \vspace{0.48cm}
        \begin{figure}[h!]
          \begin{center}
            \begin{tikzpicture}
              
              % Definitions
              \tikzstyle{Dataset} = [rectangle, rounded corners,minimum width=2cm, minimum height=1cm, text centered, draw=black]
              \tikzstyle{arrow} = [thick,->,>=stealth]

              \node (Dfa) [Dataset] {$D_{f_A}$};
              \node (D1') [Dataset, right of= Dfa, xshift=3cm] {$D_1'$};
              \node (GA) [Dataset, right of= D1', xshift=2.5cm] {$G_A$};
              \node (DA) [Dataset, right of= GA, xshift=2.5cm] {$D_A$};


              \node (Dft) [Dataset, below of =Dfa, yshift=-2.5cm] {$D_{f_t}$};
              \node (D2') [Dataset, right of= Dft, xshift=3cm] {$D_2'$};
              \node (Gs) [Dataset, right of= D2', xshift=2.5cm] {$G_s$};
              
              \draw [arrow] (Dfa) -- node[anchor=south] {subgraph} (D1');
              \draw [arrow] (D1') -- node[anchor=south] {modify} (GA);
              \draw [arrow] (GA) -- node[anchor=south] {} (DA);

              \draw [arrow] (Dft) -- node[anchor=south] {subgraph} (D2');
              \draw [arrow] (D2') -- node[anchor=south] {modify} (Gs);
              

            \end{tikzpicture}
          \end{center}
          \caption{Sampling Datasets - Different Dataset Distribution - Attack 3}
          \label{figure:sample-datasets-attack3}
        \end{figure}

        However, this time the adversary was trained on another dataset distribution than the target model.
        E.g. the adversary was trained with the \emph{Cora} dataset.
        We then use $A$ to steal links from the training graph of a target model that was trained on the \emph{CiteSeer} dataset, considering an incomplete subgraph of the \emph{CiteSeer} dataset as $G_s$.
        Because of that we use $dist_{ij}$ as input for the attacker model instead of the posterior concatenation, like it was done in \emph{Attack 2}.


  \section{Target Models}

    As our target models, we used three different types of graph neural network models, each with a slightly different algorithm used to obtain the neighborhood embeddings.

    \subsection*{GraphSAGE}
      In June 2017 Hamilton et al.\cite{hamilton2018inductive} proposed a general framework, called GraphSAGE (SAmple and aggreGatE), for inductive node embedding. 
      They came up with an idea of leveraging node features like text attributes, node profile information or node degrees to learn an embedding function that generalizes to unseen nodes instead of prior approaches that use matrix factorization.
      Until then, the training process focused on individual embeddings for each node, but with the GraphSAGE algorithm, a function is learned that generates embeddings by sampling and aggregating features from a node's neighborhood.

    \subsection*{Graph Attention Networks}
      pass

    \subsection*{Graph Convolutional Networks}
      pass

    For each   


  \section{Attacker Model}
