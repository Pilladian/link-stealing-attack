\chapter{Evaluation}

    \section{Datasets}
        In total we used 3 different Datasets (Section \refeq{section:datasets}), which are common used benchmark datasets for evaluating Graph Neural Networks \cite{acharya2019feature, gao2019graphnas, gcn}.
        All of them are citation datasets with nodes being publications and edges representing citations among these publications.        
        Furthermore do all datasets contain nodes' attributes and labels.

        The method to sample the attack dataset $D_A$, described in Section \refeq{subsection:dataset-samples}, follows the common practice in the literature of link prediction \cite{BHPZ17, grover2016node2vec}.

    \section{Metric}
        We use F1-Score as our main evaluation metric.
        It is a common used metric in binary classification \cite{lipton2014thresholding, santus2016features, woodbridge2016predicting}, since it is the harmonic mean of precision and recall.-
        The highest value, that is possible, is 1.0, indicating perfect precision and recall.
        If either the precision or the recall is zero, the F1-Score is 0.0.
        Leading to f1-Score values between 0 and 1.

        \subsection*{Precision}
            A high precision represents a high probability that the prediction of a Machine Learning model is correct.
            Let $M$ be a Machine Learning model that was trained to predict whether an email is spam or not.
            High precision means, that when the model labels an email as malicious, it is correct most of the time and vice versa.

        \subsection*{Recall}
            A high recall represents a high percentage of correctly classified inputs.
            In the example just given, that means, that $M$ is able to identify a high amount of spam-mails as malicious.

    \section{Attack Performance}
        The results presented below are the average results of 10 runs.
        Meaning, that all attacks have been performed multiple times to get a better relation.
        Note, that the performance varies based on the random choice of the nodes that are included in the partial graph $G_s$.
        Each attack is performed on all target models, which have been trained on different datasets.
        As our baseline we consider that the partial graph $G_s$ doesn't contain any links.
        We then add 20\% of the edges per attack, ending up with 80\% known edges.

        \subsection*{Attack 1}
            Like described in Section \refeq{section:attack1} this attack performs link stealing attacks on the same dataset distribution using the concatenation of the posterior outputs of two nodes to infer whether they have been connected or not. 
            Figure \refeq{figure:eval-att1-citeseer} presents the results for our link stealing attacks on the target models, that have been trained on the CiteSeer dataset.
            Please find the results for Cora in Figure \refeq{figure:eval-att1-cora} and for Pubmed in Figure \refeq{figure:eval-att1-pubmed}

            \begin{figure}[h]
                \begin{center}
                    \includegraphics[width=\textwidth]{attack-1-citeseer}
                    \caption{Attack 1 - $D_{f_t} = CiteSeer$}
                    \label{figure:eval-att1-citeseer}
                \end{center}
            \end{figure}

            Note that our baseline already achieves an average F1-Score of $0.7421$ depending on Graph Neural Network type and target dataset $D_{f_t}$.
            The baseline performed best ($0.7903$ F1-Score) on the Graph Convolutional Neural Network, when it was trained on the  CiteSeer dataset and it performed worst ($0.6663$ F1-Score) on the Graph Attention Network when it was trained on the Cora dataset.
            Intuitively, with rising amount of known edges, we would expect a higher performance of our attacks.
            However, in some cases, the performance grows until 40\% or 60\% of known edges and drops afterwards.
            More precisely, sometimes the attack performance is better with only 60\% of known edges than with 80\%.
            The reason we observe this, is the following. 
            Since we use the deleted edges as positive samples for our training data, the more edges are known, the less data will be provided for training the attack model. 
            Leading to more training data in our baseline than in our 80\%-known-edges attack.
            With respect to all performed attacks, the average F1-Score is $0.7413$.
            Table \refeq{table:attack1-best-and-worst-performance} presents the F1-Scores of our best and worst attacks.

            \vspace{0.48cm}
            \begin{table}[!h]
                \centering
                \footnotesize
                \begin{tabular}{l|l|l|l|l|}
                \toprule
                Target Model & $D_{f_t}$ & $G_A$ Distribution & $\alpha$ & F1-Score \\
                \midrule
                GCN       & Cora   & Cora   & 0.8 & $0.8110$ \\
                GraphSAGE & Pubmed & Pubmed & 0.2 & $0.5643$ \\
                
                \bottomrule
                \end{tabular}
                \caption{Attack-1: Best and Worst Attack Performance}
                \label{table:attack1-best-and-worst-performance}
            \end{table}

        \subsection*{Attack 2}
            Like described in Section \refeq{section:attack2} this attack performs link stealing attacks on the same dataset distribution using the calculated distance vector of the posterior outputs of two nodes to infer whether they have been connected or not. 
            Figure \refeq{figure:eval-att2-citeseer} presents the results for our link stealing attacks on a target model, that was trained on the CiteSeer dataset.
            Please find the results for Cora in Figure \refeq{figure:eval-att2-cora} and for Pubmed in Figure \refeq{figure:eval-att2-pubmed}

            \begin{figure}[h]
                \begin{center}
                    \includegraphics[width=\textwidth]{attack-2-citeseer}
                    \caption{Attack 2 - $D_{f_t} = CiteSeer$}
                    \label{figure:eval-att2-citeseer}
                \end{center}
            \end{figure}

            The first noticeable fact is, that using the distance vector instead of the concatenation of the posteriors is more effective.
            This time the average F1-Score of our baseline is $0.7770$, again depending on Graph Neural Network type and target dataset $D_{f_t}$.
            The baseline performed best ($0.8374$ F1-Score) on the Graph Attention Network, when it was trained on the CiteSeer dataset and it performed worst ($0.7535$ F1-Score) on the GraphSAGE GNN when it was trained on the Pubmed dataset.
            Again we can observe, that the results support our forecast.
            % calculate improvement: impr = max(20p, 40p, 60p, 80p) - baseline
            With rising amount of known edges, the attack performance grows, leading to an improvement up to $0.08$ F1-Score, while comparing the baseline performance with the results of attacks with more known edges.
            With respect to all performed attacks, the average F1-Score is $0.8049$.
            Table \refeq{table:attack2-best-and-worst-performance} presents the F1-Scores of our best and worst attacks.
            
            \vspace{0.48cm}
            \begin{table}[!h]
                \centering
                \footnotesize
                \begin{tabular}{l|l|l|l|l|}
                \toprule
                Target Model & $D_{f_t}$ & $G_A$ Distribution & $\alpha$ & F1-Score \\
                \midrule
                GAT       & CiteSeer & CiteSeer & 0.8 & $0.8923$ \\
                GraphSAGE & Cora     & Cora     & 0.8 & $0.7463$ \\
                
                \bottomrule
                \end{tabular}
                \caption{Attack-2: Best and Worst Attack Performance}
                \label{table:attack2-best-and-worst-performance}
            \end{table}

        \subsection*{Attack 3}
            Like described in Section \refeq{section:attack3} this attack performs link stealing attacks on a different dataset distribution using the calculated distance vector of the posterior outputs of two nodes to infer whether they have been connected or not. 
            The following Figures \refeq{figure:eval-att3-cora-citeseer} and \refeq{figure:eval-att3-pubmed-citeseer} present the results for our link stealing attacks on a target model, that was trained on the CiteSeer dataset while the adversary was trained on another distribution dataset.
            Please find the results for Cora in Figures \refeq{figure:eval-att3-citeseer-cora} and \refeq{figure:eval-att3-pubmed-cora} and for Pubmed in Figures \refeq{figure:eval-att3-cora-pubmed} and \refeq{figure:eval-att3-citeseer-pubmed}.

            \begin{figure}[h]
                \begin{center}
                    \includegraphics[width=\textwidth]{attack-3-cora-citeseer}
                    \caption{Attack 3 - $D_{f_t} = CiteSeer$ and $D_A = Cora$}
                    \label{figure:eval-att3-cora-citeseer}
                \end{center}
            \end{figure}

            \begin{figure}[h]
                \begin{center}
                    \includegraphics[width=\textwidth]{attack-3-pubmed-citeseer}
                    \caption{Attack 3 - $D_{f_t} = CiteSeer$ and $D_A = Pubmed$}
                    \label{figure:eval-att3-pubmed-citeseer}
                \end{center}
            \end{figure}

            Evaluating \emph{Attack 3}, we note, that the average attack performance of our baseline ($0.7562$ F1-Score) is higher than the average baseline performance of \emph{Attack 1} but lower than the average baseline performance of \emph{Attack 2}.
            The results differ, based on the used datasets $D_{f_t}$ and $D_A$ and the architecture of the target model.
            We achieve a minimum baseline attack performance of $0.6272$ F1-Score with $D_{f_t}$ being the Cora dataset, the attack model being trained on the Pubmed dataset and the target model being a Graph Convolutional Neural Network.
            However, when our target model is a Graph Attention Network, which was trained on the CiteSeer dataset, while the attack model was trained on the Pubmed dataset, we can achieve a maximum baseline attack performance of $0.8297$ F1-Score.  
            Like in the other experiments, the attack performance mostly increases, while the amount of known edges rises.
            This leads to an improvement up to $0.20$ F1-Score.
            With respect to all performed attacks, the average F1-Score is $0.8001$.
            Table \refeq{table:attack3-best-and-worst-performance} presents the F1-Scores of our best and worst attacks.
            
            \vspace{0.48cm}
            \begin{table}[!h]
                \centering
                \footnotesize
                \begin{tabular}{l|l|l|l|l|}
                \toprule
                Target Model & $D_{f_t}$ & $G_A$ Distribution & $\alpha$ & F1-Score \\
                \midrule 
                GAT & CiteSeer & Cora   & 0.8 & $0.8986$ \\
                GCN & Cora     & Pubmed & 0.0 & $0.6272$ \\
                
                \bottomrule
                \end{tabular}
                \caption{Attack-3: Best and Worst Attack Performance}
                \label{table:attack3-best-and-worst-performance}
            \end{table}
        
        The following two tables provide an overview of the best attack performances (Table \refeq{table:attack-best-results-all}) and of the average attack performances on our three target model architectures (Table \refeq{table:attack-avg-results-all}) with respect to all our three attack types.

        \vspace{0.48cm}
        \begin{table}[!h]
            \centering
            \footnotesize
            \begin{tabular}{l|l|l|l|}
                \toprule
                Target Model & Attack 1 & Attack 2 & Attack 3 \\
                \midrule
                GraphSAGE & $0.7577$ & $0.8275$ & $0.8149$ \\
                GAT & $0.8024$ & $0.8923$ & $0.8985$ \\
                GCN & $0.8110$ & $0.8732$ & $0.8777$ \\
                
                \bottomrule
            \end{tabular}
            \caption{Best Attack Performances on Target Models (F1-Score)}
            \label{table:attack-best-results-all}
          \end{table}
        
        \vspace{0.48cm}
        \begin{table}[!h]
            \centering
            \footnotesize
            \begin{tabular}{l|l|l|l|}
                \toprule
                Target Model & Attack 1 & Attack 2 & Attack 3 \\
                \midrule
                GraphSAGE & $0.6943$ & $0.7796$ & $0.7808$ \\
                GAT & $0.7529$ & $0.8251$ & $0.8210$ \\
                GCN & $0.7766$ & $0.8100$ & $0.7985$ \\
                
                \bottomrule
            \end{tabular}
            \caption{Average Attack Performances on Target Models (F1-Score)}
            \label{table:attack-avg-results-all}
        \end{table}
    
    We notice, that sometimes the results of \emph{Attack 3} are more effective than \emph{Attack 2} (Table \refeq{table:attack-best-results-all}), which is against our intuition, since \emph{Attack 3} uses different distribution datasets.
    This phenomenon can be explained with the size of $G_A$.
    In \emph{Attack 2} $G_A$ is much smaller, meaning that less training data can be provided to train the attack model.
    Since $G_A$ is a complete dataset - Cora, CiteSeer or Pubmed - in \emph{Attack 3}, the attack model has more data it can be trained on, leading to a better attack performance.
    In general however (Table \refeq{table:attack-avg-results-all}), we observe that \emph{Attack 2} scores better than \emph{Attack 3} on all target models.

    \section{Possible Defense}
        One possible defense, which already has been presented by He et al. \cite{DBLP:journals/corr/abs-2005-02131}, is to minimize the posterior output vector of $f$. 
        Meaning that instead of providing the complete posterior output of $f$'s prediction, $f$ could only present the top $k$ posteriors.
        In that way the adversary must attack the target model based on less information which makes the attack less effective.
        Since the performed attacks are very similar and only the architecture and functionality of the target model varies, we can assume, that the defense would lead to a similar drop of attack performance in our work.
        
        However, if we assume, that $f$ always provides the complete output posterior, there still exist some methods to mitigate our attacks. 
        Like He et al. proposed in their work, it is also possible to defend against these attacks by leveraging differential privacy (DP) and adversarial examples.
        More specifically we could adopt edge-Differential Privacy \cite{Hay_accurateestimation, lu2020protect, 8345716, Zhang_2015}.
        The approach of Zhang et al. \cite{Zhang_2015} specifies a probability distribution over possible outputs to ensure DP.
        While it is carefully defined to maximize the utility for the given input, it still provides the required privacy level.
        Like shown in previous work \cite{jia2020attriguard, jia2019memguard}, it is also possible to fool the adversary by adding noise to the prediction of the target model.

    \section{Summary of Results}
        To sum up, we made the following observations during our experiments.
        First, our attacks can successfully steal links from inductive trained Graph Neural Networks.
        For example we were able to steal links from Graph Convolutional Networks with F1-Scores up to $0.8955$, which shows the effectiveness of our attacks.
        Second, there exists a dependence between the amount of known edges the adversary has and the attack performance. 
        The more background knowledge the adversary has, the better the results.
        Furthermore, we also achieve good results with our transferring attack.
        However, the performance varies dependent on the shadow dataset.
        But in total the different distribution does not really impact the performance, since the results are similar to same dataset distribution attacks. 
        We observe, that the average attack performance of \emph{Attack 1} scores worst on all our target models.
        We notice an average improvement of $0.09$ F1-Score for GraphSAGE, $0.07$ F1-Score for Graph Attention Networks and $0.03$ F1-Score for Graph Convolutional Networks, when we compare the concatenation of the posteriors (\emph{Attack 1}) with the sampling of features based on the posteriors (\emph{Attack 2 and Attack 3}).
        We lastly note, that our attacks perform worst on the GraphSAGE GNN meaning, that these networks seem to be the most resistant ones.

