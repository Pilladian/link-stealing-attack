\chapter{Evaluation}

    \section{Datasets}
        In total we used 3 different Datasets (Section \refeq{section:datasets}), which are common used benchmark datasets for evaluating graph neural networks \cite{acharya2019feature, gao2019graphnas, gcn}.
        All of them are citation datasets with nodes being publications and edges representing citations among these publications.        
        Furthermore do all datasets contain nodes' attributes and labels.

        The method to sample the attack dataset $D_A$, described in Section \refeq{subsection:dataset-samples}, follows the common practice in the literature of link prediction \cite{BHPZ17, grover2016node2vec}.

    \section{Metric}
        We use F1-Score as our main evaluation metric.
        It is a common used metric in binary classification \cite{lipton2014thresholding, santus2016features, woodbridge2016predicting}, since it is the harmonic mean of precision and recall.-
        The highest value, that is possible, is 1.0, indicating perfect precision and recall.
        If either the precision or the recall is zero, the F1-Score is 0.0.
        Leading to f1-Score values between 0 and 1.

        \subsection*{Precision}
            A high precision represents a high probability that the prediction of a model is correct.
            Let $M$ be a machine learning model that was trained to predict whether an email is spam or not.
            High precision means, that when the model labels an email as malicious, it is correct most of the time and vice versa.

        \subsection*{Recall}
            A high recall represents a high percentage of correctly classified inputs.
            In the example just given, that means, that $M$ is able to identify a high amount of spam-mails as malicious.


    \section{Attack Performance}
        The results presented below are the average results of 10 runs.
        Meaning, that all attacks have been performed multiple times to get a better relation.
        Note, that the performance varies based on the random choice of the nodes that are included in the partial graph $G_s$.
        Each attack is performed on all target models, which have been trained on different datasets.
        As our baseline we consider that the partial graph $G_s$ doesn't contain any links.
        We then add 20\% of the edges per attack, ending up with 80\% known edges.

        \subsection*{Attack 1}
            Like described in Section \refeq{section:attack1} this attack performs link stealing attacks on the same dataset distribution using the concatenation of the posterior outputs of two nodes to infer whether they have been connected or not. 
            Figure \refeq{figure:eval-att1-citeseer} presents the results for our link stealing attacks on the target models, that have been trained on the CiteSeer dataset.
            Please find the results for Cora in Figure \refeq{figure:eval-att1-cora} and for Pubmed in Figure \refeq{figure:eval-att1-pubmed}

            \begin{figure}[h]
                \begin{center}
                    \includegraphics[width=\textwidth]{attack-1-citeseer}
                    \caption{Attack 1 - $D_{f_t} = CiteSeer$}
                    \label{figure:eval-att1-citeseer}
                \end{center}
            \end{figure}

            Note that our baseline already achieves an average F1-Score of $0.7278$ depending on graph neural network type and target dataset $D_{f_t}$.
            The baseline performed best ($0.7605$ F1-Score) on the graphsage graph neural network, when it was trained on the  CiteSeer dataset and it performed worst ($0.6490$ F1-Score) on the graph attention network when it was trained on the Cora dataset.
            Intuitively, with rising amount of known edges, we would expect a higher performance of our attacks.
            However, in most cases, the performance grows until 40\% or 60\% of known edges and drops afterwards.
            More precisely, in almost every case, the attack performance is better with only 60\% of known edges than with 80\%.
            The reason we observe this, is the following. Since we use the deleted edges as positive samples for our training data, the more edges are known, the less data will be provided for training the adversary. Leading to more training data in our baseline than in our 80\%-known-edges attack.
            With respect to all performed attacks, the average F1-Score is $0.7404$, with the lowest being $0.4130$ (GCN trained on Pubmed with 80\% known edges) and the highest being $0.810$ (GraphSAGE trained on CiteSeer with 60\% known edges).

        \subsection*{Attack 2}
            Like described in Section \refeq{section:attack2} this attack performs link stealing attacks on the same dataset distribution using the calculated distance vector of the posterior outputs of two nodes to infer whether they have been connected or not. 
            Figure \refeq{figure:eval-att2-citeseer} presents the results for our link stealing attacks on a target model, that was trained on the CiteSeer dataset.
            Please find the results for Cora in Figure \refeq{figure:eval-att2-cora} and for Pubmed in Figure \refeq{figure:eval-att2-pubmed}

            \begin{figure}[h]
                \begin{center}
                    \includegraphics[width=\textwidth]{attack-2-citeseer}
                    \caption{Attack 2 - $D_{f_t} = CiteSeer$}
                    \label{figure:eval-att2-citeseer}
                \end{center}
            \end{figure}

            The first noticeable fact is, that using the distance vector instead of the concatenation of the posteriors is much more effective.
            This time the average F1-Score of our baseline is $0.7715$, again depending on graph neural network type and target dataset $D_{f_t}$.
            The baseline performed best ($0.8355$ F1-Score) on the graphsage graph neural network, when it was trained on the  CiteSeer dataset and it performed worst ($0.6996$ F1-Score) on the graph convolutional network when it was trained on the Cora dataset.
            This time, the results support our forecast.
            % calculate improvement: impr = max(20p, 40p, 60p, 80p) - baseline
            With rising amount of known edges, the attack performance grows, leading to an improvement up to $0.06$ F1-Score, while comparing the baseline performance with the results of attacks with known edges.
            With respect to all performed attacks, the average F1-Score is $0.7969$, with the lowest being $0.6996$ (GCN trained on Cora with 0\% known edges) and the highest being $0.8909$ (GAT trained on CiteSeer with 80\% known edges).

        \subsection*{Attack 3}
            Like described in Section \refeq{section:attack3} this attack performs link stealing attacks on a different dataset distribution using the calculated distance vector of the posterior outputs of two nodes to infer whether they have been connected or not. 
            The following Figures \refeq{figure:eval-att3-cora-citeseer} and \refeq{figure:eval-att3-pubmed-citeseer} present the results for our link stealing attacks on a target model, that was trained on the CiteSeer dataset while the adversary was trained on another distribution dataset.
            Please find the results for Cora in Figures \refeq{figure:eval-att3-citeseer-cora} and \refeq{figure:eval-att3-pubmed-cora} and for Pubmed in Figures \refeq{figure:eval-att3-cora-pubmed} and \refeq{figure:eval-att3-citeseer-pubmed}.

            \begin{figure}[h]
                \begin{center}
                    \includegraphics[width=\textwidth]{attack-3-cora-citeseer}
                    \caption{Attack 3 - $D_{f_t} = CiteSeer$ and $D_A = Cora$}
                    \label{figure:eval-att3-cora-citeseer}
                \end{center}
            \end{figure}

            \begin{figure}[h]
                \begin{center}
                    \includegraphics[width=\textwidth]{attack-3-pubmed-citeseer}
                    \caption{Attack 3 - $D_{f_t} = CiteSeer$ and $D_A = Pubmed$}
                    \label{figure:eval-att3-pubmed-citeseer}
                \end{center}
            \end{figure}

            Evaluating \emph{Attack 3}, we note, that the average attack performance of our baseline ($0.7668$} F1-Score) is higher than the average baseline performance of \emph{Attack 1} but lower than the average baseline performance of \emph{Attack 2}.
            The results differ, based on the used datasets $D_{f_t}$ and $D_A$ and the architecture of the target model.
            We achieve a minimum baseline attack performance of $0.6636$ F1-Score with $D_{f_t}$ being the Cora dataset, the attack model being trained on Pubmed and the target model being a graph convolutional neural network.
            However, when our target model is a graph attention network, which was trained on the CiteSeer dataset, while we set $D_A = Pubmed$, we can achieve a maximum baseline attack performance of $0.8383$ F1-Score.  
            Like in \emph{Attack 2}, the attack performance increases as well, while the amount of known edges rises.
            This leads to an improvement up to $0.1320$ F1-Score.
            With respect to all performed attacks, the average F1-Score is $0.7938$, with the lowest being $0.6636$ (GCN trained on Cora and adversary trained on Pubmed with 0\% known edges) and the highest being $0.8854$ (GAT trained on CiteSeer and adversary trained on Cora with 80\% known edges).
        
        The following two tables provide an overview of the best attack performances (Table \refeq{table:attack-best-results-all}) and of the average attack performances on our three target model architectures (Table \refeq{table:attack-avg-results-all})

        \vspace{0.48cm}
        \begin{table}[!h]
            \centering
            \footnotesize
            \begin{tabular}{l|l|l|l|}
              \toprule
              Target Model & Attack 1 & Attack 2 & Attack 3 \\
              \midrule
              GraphSAGE & $0.8107$ & $0.8774$ & $0.8793$ \\
              GAT & $0.8064$ & $0.8909$ & $0.8854$ \\
              GCN & $0.7603$ & $0.8116$ & $0.8092$ \\
            
              \bottomrule
            \end{tabular}
            \caption{Best Attack Performances on Target Models (F1-Score)}
            \label{table:attack-best-results-all}
          \end{table}
        
        \vspace{0.48cm}
        \begin{table}[!h]
            \centering
            \footnotesize
            \begin{tabular}{l|l|l|l|}
            \toprule
            Target Model & Attack 1 & Attack 2 & Attack 3 \\
            \midrule
            GraphSAGE & $0.7642$ & $0.8150$ & $0.8139$ \\
            GAT & $0.7549$ & $0.8248$ & $0.8199$ \\
            GCN & $0.7020$ & $0.7510$ & $0.7477$ \\
              
            \bottomrule
            \end{tabular}
            \caption{Average Attack Performances on Target Models (F1-Score)}
            \label{table:attack-avg-results-all}
        \end{table}
            
    
    \section{Possible Defense}
        One possible defense, which already has been presented by He et al. \cite{DBLP:journals/corr/abs-2005-02131}, is to minimize the posterior output vector of $f$. 
        Meaning that instead of providing the complete posterior output of $f$'s prediction, $f$ could only present the top $k$ posteriors.
        In that way the adversary must attack the target model based on less information which makes the attack less effective.
        Since the performed attacks are very similar and only the architecture and functionality of the target model varies, we can assume, that the defense would lead to a similar drop of attack performance in our work.
        
        However, if we assume, that $f$ always provides the complete output posterior, there still exist some methods to mitigate our attacks. 
        Like He et al. proposed in their work, it is also possible to defend against these attacks by leveraging differential privacy (DP) and adversarial examples.
        More specifically we could adopt edge-Differential Privacy \cite{Hay_accurateestimation, lu2020protect, 8345716, Zhang_2015}.
        The approach of Zhang et al. \cite{Zhang_2015} specifies a probability distribution over possible outputs to ensure DP.
        While it is carefully defined to maximize the utility for the given input, it still provides the required privacy level.
        Like shown in previous work \cite{jia2020attriguard, jia2019memguard}, it is also possible to fool the adversary by adding noise to the prediction of the target model.

    \section{Summary of Results}
        To sum up, we made the following observations during our experiments.
        First, our attacks can successfully steal links from inductive trained graph neural networks.
        For example we were able to steal links from graph attention networks with F1-Scores up to $0.8908$, which shows the effectiveness of our attacks.
        Second, there exists a dependence between the amount of known edges the adversary has and the attack performance. 
        The more background knowledge the adversary has, the better the results.
        Lastly, we also achieve good results with our transferring attack.
        However, the performance varies dependent on the shadow dataset.
        But in total the different distribution does not really impact the performance, since the results are similar to same dataset distribution attacks. 

