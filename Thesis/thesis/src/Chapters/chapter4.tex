\chapter{Attacks}

  In recent work He et al. \cite{DBLP:journals/corr/abs-2005-02131} proposed the first link stealing attacks on graph neural networks.
  They focused on stealing links of the graph, that was used for training the given target model.
  Like described in Section 3.3.1 this is an attack on transductive trained graph neural networks.
  In our work, we want to show, that it is possible for an adversary to steal links from any graphs, given black-box access to an inductive trained target graph neural network model.

  \section{Adversary's Goal}

    Let $f$ be the target graph neural network model and $G_s = (V_s, E_s)$ a graph with $|V_s|$ nodes and $|E_s|$ edges. 
    We assume, that some of $G_s$'s links/edges are missing.
    The goal of an adversary $A$ is, to infer whether two nodes $i,j \in V_s$ are connected to each other or not.
    More precisely, whether the link $(i,j)$ between the nodes $i$ and $j$ is missing, or does not exist.
  
  \section{Threat Model}

    For any of our attacks, we assume, \emph{Black-Box Access} (Query Access) to the target graph neural network model $f$, that was trained on a graph dataset $D_t$.
    We consider $f_A$ another graph neural network model, which was trained on a graph dataset $D_{f_A}$.
    The adversary $A$ was trained on a dataset $D_A$ to perform link stealing attacks.
    We assume, that in any attack $D_t$ and $G_s$ are from the same dataset distribution.
    However, $D_{f_A}$ must not be from the same dataset distribution as $D_t$.

    \begin{table}[!h]
      \centering
      \footnotesize
      \begin{tabular}{l|l|}
        \toprule
        Notation & Description \\
        \midrule
        $f$ & Target GNN Model \\
        $D_t$ & Dataset used to train $f$ \\
        $A$ & Adversary \\
        $D_A$ & Dataset used to train $A$ \\
        $f_A$ & GNN Model, that is involved in training $A$ \\
        $D_{f_A}$ & Dataset used to train $f_A$ \\
        $G_s$ & Incomplete Graph, $A$ performs link stealing attacks on\\
        \bottomrule
      \end{tabular}
      \caption{Attack Notations}
      \label{table:notations}
    \end{table}

  \section{Attack Methodology}

    % TODO

    % Let $f$ be the target graph neural network model and $G_s = (V_s, E_s)$ a graph with $|V_s|$ nodes and $|E_s|$ edges.
    % We assume that $E_s$ is not complete. 
    % More precisely, there exists an edge $(i,j)$ between the nodes $i,j \in V_s$, but $(i,j) \not\in E_s$.
    % The adversary $A$ queries $f$ on both nodes $i$ and $j$, obtaining $post_i = f(G_s, i)$ and $post_j = f(G_s, j)$.

    % In \emph{Attack 1} we consider $D_A$ and $D_t$ from the same dataset distribution.
    % Meaning, that both datasets share parameters like feature amount and more important number of classes. 
    % This leads to the same sized output posteriors when querying $f$ and $f_a$.
    % Based on this assumption, $A$ can directly be trained on the posteriors generated by $f_a$, since $|f_a(D_A, u)| = |f(G_s, v)|$, with $u \in V_{a}$ and $v \in V_s$. 
    % Therefor we concatenate $post_i$ and $post_j$ obtaining the input $post_{ij} = cat(post_i, post_j)$, with $cat(A,B) = [a_0,...,a_n,b_0,...,b_n]$, where $A = [a_0,...,a_n]$ and $B = [b_0,...,b_n]$.
    % Given $post_{ij}$, $A$ now can infer, whether $i$ and $j$ have been connected and the edge is missing in $G_s$ or not.

    % In \emph{Attack 3} we consider $D_A$ and $D_t$ from different dataset distributions.
    % That means, that the target graph neural network model $f$ and the graph neural network model $f_a$, that was used to train the adversary, must not share parameters like the numbers of classes.
    % Therefore, we must assume that they are different.
    % So it is not possible anymore to train the adversary directly on the posterior output of $f$, since $|f_a(D_A, u)| \neq |f(G_s, v)|$, with $u \in V_{a}$ and $v \in V_s$.
    % Based on this assumption, we need to sample the input for $A$, by creating features based on the posteriors, instead of using them directly.
    % As features we use eight common distance metrics, to measure the distance between $post_i$ and $post_j$.
    % We have in total experimented with Cosine distance, Euclidean distance, Correlation distance, Chebyshev distance, Braycurtis distance, Canberra distance, Manhattan distance, and Square-euclidean distance.
    % The formal definition of each distance metrics is listed in table \refeq{table:distance}.
    % So we construct the input $dist_{ij}$ for $A$ as follows: $dist_{ij} = dist(post_i, post_j)$, where $dist(post_i, post_j) = [Cosine(post_i,post_j), ..., Sqeuclidean(post_i,post_j)]$ 

    However, for \emph{Attack 2}, we sample the input for $A$, like it was done in \emph{Attack 3}, to better compare the impact of the dataset distribution on our attacks. 

    Furthermore, for each Attack we assume different amounts of edges given in $D_A$. 
    The amount of known edges is denoted as $\alpha$ and has an impact on the prediction (posteriors) of the target models $f$ and $f_a$.
    Therefor we construct new adversary graphs: $D_A^\alpha = (V_{a}^\alpha, E_{a}^\alpha)$ with $|E_{a}^\alpha| = \alpha * |E_{a}|$. 
    As different stages we define $\alpha = 0.0, 0.2, 0.4, 0.6, 0.8$. 
    The first case, $\alpha = 0.0$ represents an adversary graph $D_A^{0.0} = (V_{a}^{0.0}, E_{a}^{0.0})$ without any edges / no knowledge of the relationships between the nodes.
    $\alpha = 0.8$ leads to an adversary graph $D_A^{0.8} = (V_{a}^{0.8}, E_{a}^{0.8})$ with almost every edge considered to be known.

    The following table shows all three attacks with the dataset distribution, the input for the adversary $A$ and the feature amount.

    \begin{table}[!h]
      \centering
      \footnotesize
      \begin{tabular}{l|c|c|c}
        \toprule
        Attacks & Dataset Distribution & Input for $A$ & $A$'s Feature Amount\\
        \midrule
        Attack 1 & Same & $inp_{ij} = cat(post_i, post_j)$ & $|inp_{ij}| = |post_i| + |post_j| = 2 * |post_i|$ \\
        Attack 2 & Same & $inp_{ij} = dist(post_i, post_j)$ & $|inp_{ij}| = 8$ \\
        Attack 3 & Different & $inp_{ij} = dist(post_i, post_j)$ & $|inp_{ij}| = 8$ \\
        \bottomrule
      \end{tabular}
      \caption{Attack Methodology}
      \label{table:attack-methodology}
    \end{table}

    

    
  