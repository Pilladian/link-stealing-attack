\chapter{Attacks}

  In recent work He et al. \cite{DBLP:journals/corr/abs-2005-02131} proposed the first link stealing attacks on graph neural networks.
  They focused on stealing links of the graph, that was used for training the given target model.
  Like described in Section 3.3.1 this is an attack on transductive trained graph neural networks.
  In our work, we want to show, that it is possible for an adversary to steal links from any graphs, given black-box access to an inductive trained target graph neural network model.

  \section{Adversary's Goal}

    Let $f$ be the target graph neural network model and $G_{adv} = (V_{adv}, E_{adv})$ a graph with $|V_{adv}|$ nodes and $|E_{adv}|$ edges. 
    We assume, that some of $G_{adv}$'s links/edges are missing.
    The goal of an adversary $a$ is, to infer whether two nodes $i$ and $j$ are connected to each other or not.
    More precisely, whether the link $(i,j)$ between the nodes $i$ and $j$ is missing or does not exist.
  
  \section{Threat Model}

    To perform any of our attacks, we assume, \emph{Black-Box Access} (Query Access) to the target graph neural network model $f$, that was trained on a graph $G_{target} = (V_{target}, E_{target})$.
    Furthermore, the adversary $a$ has access to a graph $G_{adv} = (V_{adv}, E_{adv})$, of which it want's to steal links from. 
    In \emph{Attack 1} described in Section \TODO{number} and \emph{Attack 2} discussed in Section \TODO{number} the adversary's graph $G_{adv}$ is from the same dataset distribution like $G_{target}$. 
    That's why, we can train $a$ using $f$.
    In \emph{Attack 3} covered in Section \TODO{number}, we assume a different dataset distribution for $G_{adv}$.
    Considering this, we need to train a shadow model $f'$, which then can be used to train $a$.

  \section{Attack Methodology}

    Let $f$ be the target graph neural network model and $G_{adv} = (V_{adv}, E_{adv})$ a graph with $|V_{adv}|$ nodes and $|E_{adv}|$ edges.
    We assume that $E_{adv}$ is not complete. 
    More precisely, there exists an edge $(i,j)$ between the nodes $i,j \in V_{adv}$, but $(i,j) \not\in E_{adv}$.
    The adversary $a$ queries $f$ on both nodes, obtaining $post_i = f(G_{adv}, i)$ and $post_j = f(G_{adv}, j)$.

    In \emph{Attack 1} we consider $G_{adv}$ and $G_{target}$ from the same dataset distribution.
    That's why $a$ can directly be trained on the posteriors. 
    Therefor we concatenate $post_i$ and $post_j$ obtaining the input $post_{ij} = cat(post_i, post_j)$, with $cat(A,B) = [a_0,...,a_n,b_0,...,b_n]$, where $A = [a_0,...,a_n]$ and $B = [b_0,...,b_n]$.
    Given $post_{ij}$, $a$ now can infer, whether $i$ and $j$ have been connected and the edge is missing in $G_{adv}$ or not.

    In \emph{Attack 3} we consider $G_{adv}$ and $G_{target}$ from different dataset distributions.
    The target graph neural network model $f$ has been trained on $G_{target}$, having $|f(G_{target},u)|$ classes, where $u \in V_{target}$.
    The size of the concatenation of two posteriors obtained from $f$ will be $|cat_{f}| = |cat(f(G_{target},u), f(G_{target},v))|$, where $u,v \in V_{target}$. 
    The shadow model $f'$ however, has been trained on $G_{adv}$, having $|f'(G_{adv},u)|$ classes, where $u \in V_{adv}$.
    The size of the concatenation of two posteriors obtained from $f'$ will be $|cat_{f'}| = |cat(f'(G_{adv},u), f'(G_{adv},v))|$, where $u,v \in V_{adv}$. 
    Since we must assume $|cat_{f}| \neq |cat_{f'}$, we need to sample the input for $a$, creating features based on the posteriors, instead of using them directly.
    As features we use eight common distance metrics, to measure the distance between $post_i$ and $post_j$.
    We have in total experimented with Cosine distance, Euclidean distance, Correlation distance, Chebyshev distance, Braycurtis distance, Canberra distance, Manhattan distance, and Square-euclidean distance.

    \begin{table}[!h]
      \centering
      \label{table:distance}
      \footnotesize
      \begin{tabular}{l|c}
        \toprule
        Metrics & Definition \\
        \midrule
        Cosine & $1 - \dfrac{f(u)\cdot f(v)}{\left|f(u)\right|2\left|f(v)\right|_2}$ \\
        Euclidean & $\left|f(u) - f(v)\right|_2$ \\
        Correlation & $1-\dfrac{(f(u)-\overline{f(u)}) \cdot(f(v)-\overline{f(v)})}{|(f(u)-\overline{f(u)})|{2}|(f(v)-\overline{f(v)})|{2}}$ \\
        Chebyshev & $\max _{i}\left|f_i(u)-f_i(v)\right|$ \\
        Braycurtis & $\dfrac{\sum\left|f_i(u)-f_i(v)\right|} {\sum\left|f_i(u)+f_i(v)\right|}$ \\
        Manhattan & $\sum{i}\left|f_i(u)-f_i(v)\right|$ \\
        Canberra & $\sum_{i} \dfrac{\left|f_i(u)-f_i(v)\right|}{\left|f_i(u)\right|+\left|f_i(v)\right|}$ \\
        Sqeuclidean & $\left|f(u) - f(v)\right|_2^2$ \\
        \bottomrule
      \end{tabular}
      \caption{Distance metrics: $f_i(u)$ represents the $i$-th component of $f(u)$.}
    \end{table}

    

    
  