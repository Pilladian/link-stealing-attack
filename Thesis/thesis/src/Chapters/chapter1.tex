\chapter{Introduction}

	% Figure Example
	% \begin{figure}
		% 	\lstinputlisting[language=C, firstline=\interestingstart, lastline=\interestingend]{\somecfile}
		% 	\caption{caption}
		% 	\label{code:aes_unsealdata}
		% \end{figure}

	\section{Motivation}
		% \TODO{Update: Not all inductive settings use aggregation functions}
		% \TODO{Transductive (use full graph) - Inductive (use Local graph / neighborhood)}

		A graph is a data structure which is used to model large data and the relationships between their entities \cite{DBLP:journals/corr/abs-2005-00687, cook2006mining}.
		Graphs consist of nodes and edges and can be used to model data in almost every domain.
		For example in social networks, healthcare analytics or protein-protein interactions.
		In a social network, the nodes would be the users that are registered int the network and the edges would represent whether the users know each other or not.
		If they know each other, the two nodes will be linked in the graph and if they don't know each other, the link does not exist.
		A graph itself can be deemed as intellectual property of the data owner, since one may spent lots of time and resources collecting and preparing the data.
		In most cases the graph is also highly confidential because it contains sensitive information like private social relationships between users in a social network or medical information about specific patients in healthcare-analytic datasets.
		Since nowadays graphs are a common way to store and visualize data, Machine Learning algorithms have been improved to directly operate on them.
		These Machine Learning Models are called Graph Neural Networks (GNNs) \cite{atwood2016diffusionconvolutional, defferrard2017convolutional}.
		They can be used in different tasks to directly operate on graphs.
		For example they can be trained to perform node classification \cite{kipf2017semisupervised}.
		More precisely, given a graph containing a few labeled nodes the model is trained to predict the labels of the other unlabeled nodes in the graph by considering the graph structure, feature vectors or links of the nodes.
		They can also be used to perform link prediction, where GNNs are trained to predict whether to people are linked or not.
		In social networks we call this friendship prediction \cite{zhang2018link}.

		A Graph Neural Network can be trained in different ways, depending on the purpose it will be used later.
		One way is to train them transductive \cite{5206871, ZHA2010187, WANG2017218, 10.1007/978-3-642-04174-7_29}.
		In the transductive setting, we consider the graph to be fixed.
		Meaning that neither the edges nor the feature vectors of the nodes change during the lifetime of the graph.
		Regarding the node classification problem that means, that we have some labeled nodes, we use for training, and many unlabeled nodes, we want to classify correctly.
		Nevertheless this training method is possible, it hardly can be applied to real world problems like training a graph neural network on social network data.
		That's why graphs in most cases keep evolving.
		E.g. in social networks, every day new users register to the network while others delete their accounts.
		To address this problem GNNs can also be trained using the inductive setting \cite{zeng2020graphsaint, 8519335, zhang2020document}.
		More specifically, instead of providing a fixed graph as input and train the model to learn the local graph structure, we now want the model to learn an aggregation and update function.
		These two functions are used to update a nodes feature vector with the aggregation of its neighborhood.
		In that way, only a partial graph is used for training the model instead of considering the full graph structure.
		With the inductive setting, the model can generalize to unseen nodes, by aggregating their neighborhood, updating the nodes feature vector and querying the model on the updated result.
		In that way it is now possible to update the GNN model on new nodes without retraining it over and over again.

		In our work, we want to show, that inductive trained graph neural networks are very likely to leak sensitive information about their training graph. 
		Meaning that queries on a partial graph of the training graph can reveal links, that are deemed confidential and thus lead to a big privacy risk.

	\section{Outline}
		This thesis starts giving an overview of machine learning and some privacy breaching attacks, like membership inference attacks or model inversion attacks, that have been developed over the years.
		Especially considering graphs and graph neural networks we take a first look at link stealing attacks that have been proposed the first time in 2020.
		We will then provide some background information about graphs, neural networks, graph neural networks and link stealing attacks.
		We describe the intuition behind those concepts, where and why we use graph neural networks and what the difference between transductive and inductive training is.
		After that we propose our attacks against inductive trained graph neural networks, which aim to steal links from the target models training graph, talking about their intuition, their functionality and implementation.
		At the end, we will evaluate our findings by presenting our experimental results and discuss their impact.
		We also provide some ideas about future work, that might be interesting.