\chapter{Background}

  \section{Neural Networks}

    Neural Networks (NNs) are key components in Artificial Intelligence (AI) and Deep Learning.
    They try to simulate some properties and the functionality of biological neural networks, like our brain, by imitating the way biological neural systems process data.
    Today, they have been applied successfully to speech recognition, face recognition on images or the transformation from speech to text.
    They are used to model software agents in video games, let autonomous robots learn new things or find patterns in data.

    A neural network consists of multiple layers.
    An input layer, one or many more hidden layers and an output layer.
    Each of these layers contain multiple neurons, which are represented as mathematical functions, that take multiple inputs and use them to calculate one output.
    Such neuron is also called perceptron, while fully connected neural networks are called multilayer perceptron (MLP).

    \vspace{0.48cm}
    \begin{figure}[!h]
      \begin{center}
        \begin{tikzpicture} [node distance = 2cm,
          on grid,
          %auto,
          %every state/.style = {draw = blue, fill = blue!30},
          every initial by arrow/.style = {font = \large,
              thick,-stealth
          }
          ]
          
          % Input Layer
          \node (i1) [state, initial, initial text= $input_1$] at (0,1) {};
          \node (i2) [state, initial, initial text= $input_2$] at (0,-1){};

          % Hidden Layer 1
          \node (h11) [state] at (2,2) {};
          \node (h12) [state] at (2,0) {};
          \node (h13) [state] at (2,-2) {};

          % Hidden Layer 2
          \node (h21) [state] at (4,2) {};
          \node (h22) [state] at (4,0) {};
          \node (h23) [state] at (4,-2) {};

          % Hidden Layer 2
          \node (h31) [state] at (6,2) {};
          \node (h32) [state] at (6,0) {};
          \node (h33) [state] at (6,-2) {};
          
          % Output Layer
          \node (o1) [state, accepting right, accepting text = {$output_1$}] at (8,1) {};
          \node (o2) [state, accepting right, accepting text = {$output_2$}] at (8,-1){};

          % Links
          \path [-stealth, thick]
            (i1) edge node {}   (h11)
            (i1) edge node {}   (h12)
            (i1) edge node {}   (h13)
            (i2) edge node {}   (h11)
            (i2) edge node {}   (h12)
            (i2) edge node {}   (h13)
            
            (h11) edge node {}   (h21)
            (h11) edge node {}   (h22)
            (h11) edge node {}   (h23)
            (h12) edge node {}   (h21)
            (h12) edge node {}   (h22)
            (h12) edge node {}   (h23)
            (h13) edge node {}   (h21)
            (h13) edge node {}   (h22)
            (h13) edge node {}   (h23)
            
            (h21) edge node {}   (h31)
            (h21) edge node {}   (h32)
            (h21) edge node {}   (h33)
            (h22) edge node {}   (h31)
            (h22) edge node {}   (h32)
            (h22) edge node {}   (h33)
            (h23) edge node {}   (h31)
            (h23) edge node {}   (h32)
            (h23) edge node {}   (h33)

            (h31) edge node {}   (o1)
            (h31) edge node {}   (o2)
            (h32) edge node {}   (o1)
            (h32) edge node {}   (o2)
            (h33) edge node {}   (o1)
            (h33) edge node {}   (o2);
        
        \end{tikzpicture}
      \end{center}

      \caption{Multilayer Perceptron}
      \label{figure:neural-network}
    \end{figure}

    \subsubsection*{Train a Neural Network}

      When we are confronted to a new task, we try to gain as much information as possible and based on them, we learn how to solve the problem.
      Neural networks behave similar.
      Before we can apply a neural network on classifying whether the object we provide is a spoon or fork, we need to tell the network, based on which information it should make its decision.
      That could be images of spoons and forks or their weight, length and width.
      We call this data \emph{Training Data}.
      To train the model in our example, we provide an image of a spoon and let the network make its decision.
      If the decision is correct, we wont change anything.
      But if the decision is incorrect, we need to slightly modify the weights - the connections between the perceptrons - to let the model behave different in the future.
      We do this for each image in our training set and repeat this process $n$ times (for $n$ epochs).
      After the training, we hopefully have a good trained neural network, which performs well on the provided task.

	\section{Graphs}

		As Graph we denote a data structure that contains nodes and edges. 
    Let $G = (V, E)$ be a graph with $V$ being the set of nodes and $E$ being the set of edges.
    We denote $\overrightarrow{a}$ as the feature vector of node $a$, representing the attributes of $a$.
    An edge $e = (i,j)$ contains the source node $i$ and the destination node $j$.
    In that way, links describe the relationship between the source and destination node. 
    The most popular example where graphs are used to model data, are social networks. 
    The nodes represent the users that have multiple attributes like location, gender, work place etc., while the edges state which relationship the users have.
    Lets consider a directed graph $G = (V,E)$ with $V$ representing the users and $E$ their relationships.
    If user $a$ follows user $b$, the edge $e_{ab} = (a,b)$ would be an element in $E$.
    If user $b$ follows user $a$, the edge $e_{ba} = (b,a)$ would be an element in $E$.
    Lets consider an undirected graph $G' = (V',E')$ with $V'$ representing the users and $E'$ their relationships.
    In $G'$ it doesn't matter whether user $a'$ follows $b'$ or the other way around. 
    Both results in $e'_{a'b'}, e'_{b'a'} \in E'$.
    Figure \refeq{figure:undirected-graph} shows an undirected graph.
    Since there is a link drawn between node $A$ and node $B$, we know, that there exists some relationship between them, but we don't know who follows whom ($e_{AB}, e_{BA} \in E$).
    The same holds for $e_{CF}, e_{FC} \in E$ or $e_{EG}, e_{GE} \in E$.
    Since there isn't a link drawn between node $E$ and node $C$, there doesn't exists a known relationship between them ($e_{EC}, e_{CE} \not\in E$).

    \vspace{0.48cm}
    \begin{figure}[!h]
      \begin{center}
        \begin{tikzpicture} [node distance = 2cm,
          on grid,
          %auto,
          %every state/.style = {draw = orange, fill = orange!30},
          every initial by arrow/.style = {font = \large,
              thick,-stealth
          }
          ]
          
          \node (a) [state] at (-1,2) {$A$};
          \node (b) [state] at (0,-1){$B$};
          \node (c) [state] at (3,1){$C$};
          \node (d) [state] at (5,-1){$D$};
          \node (e) [state] at (1,3){$E$};
          \node (f) [state] at (3,-2){$F$};
          \node (g) [state] at (7,3){$G$};

          % Links
          \draw (a) -- (b);
          \draw (d) -- (c);
          \draw (f) -- (a);
          \draw (a) -- (e);
          \draw (b) -- (f);
          \draw (c) -- (f);
          \draw (c) -- (g);
          \draw (d) -- (g);
          \draw (a) -- (c);
          \draw (e) -- (g);
        
        \end{tikzpicture}
      \end{center}

      \caption{Undirected Graph}
      \label{figure:undirected-graph}
    \end{figure}

	\section{Graph Neural Networks}
      
    % Describe how to query a graph neural network (f(Graph, node))
    In the last decades, social networks have become a huge part of life for many people all around the world.
    The companies behind these networks collect tons of data of each user every day.
    Let $G = (V, E)$ be a graph that models such a social network. 
    The set of all users is given as $V$ and $E$ represents their relationships.
    The feature vector $\overrightarrow{a}$ of user $a$ contains all information the company already has regarding this user.
    That could be the name, relationship status, workplace, address, amount of children and so on.
    Lets consider one attribute (\emph{workplace}) as non mandatory for the registration.
    This leads to some users $v_{workplace} = \{v $ | $ \forall v \in V: v$ has $workplace$ given$\}$, the company knows the workplace from and some users $v_{unknown} = \{v $ | $ \forall v \in V: v$ has $workplace$ not given$\}$, where the attribute is unknown.
    Based on the information $v_{workplace}$ provides, a graph neural network model $f$ can be trained to predict the missing attribute of all users in $v_{unknown}$.
    This is a simple node classification task, where the label is the missing attribute $worplace$.
    This example is one of many tasks a graph neural network can perform.
    Other options could be graph classification, where $f$ should be able to predict whether a given graph is a subgraph of another one, or link prediction, which is used for friendship prediction in social networks.
    There exist two major ideas of training a graph neural network - transductive or inductive.
    In our experiments introduced and described in Chapter \refeq{chapter:attacks} we focus on attacks on inductive trained graph neural networks. 

    \subsection*{Transductive Setting}

      In the transductive setting \cite{kipf2017semisupervised} the complete graph is provided as input to a graph neural network model $f$.
      This includes all nodes, their links and their feature vectors.
      The model learns hidden layer representations that encode both local graph structure and features of nodes to perform its task.
      In the example given above, we want to predict the $workplace$ of unlabeled users.
      Since this setting operates on the full graph, we consider the information the graph provides as known all the time, especially during the training phase.
      This implies, that all feature vectors are visible for $f$ during training.
      %include the feature vectors of the unlabeled users during the training phase as well.
      Meaning, that if new users join the network, $f$ needs to be retrained because the graph changed, making it really hard to apply this setting to real world problems.
      Datasets like social networks change every day, such that a model which is trained transductive, must be retrained as often as the structure changes, to maintain their accuracy.
      This leads to high computational costs and time constraints and thus is not very practical for very large, constant changing datasets.
      Based on this problem another learning method can be used, which does not use the full graph for training but uses small subgraphs instead.

    \subsection*{Inductive Setting}

      For inductive learning, we don't use the whole graph but instead use a neighborhood function $neighborhood(n)$ which aggregates the feature vectors of the $k$-hop neighborhood of the node $n$ to obtain the input for $f$. 
      We can set $k$ to any number.
      $k = 0$ does only consider $n$'s feature vector as input and no aggregated neighbor embeddings.
      $k = 1$ aggregates the neighbor vectors of $n$ with $n$'s feature vector, while $k = 2$ also considers the neighbors of the neighbors of $n$.
      Thanks to this algorithms, it's no longer necessary to train $f$ on the whole graph but instead only use the neighborhood of a specific node.
      This approach can be used to train $f$ on one graph, where all information are known and apply it on other graphs, where we want to perform node classification on.
      This is possible since the model does not learn a graph representation but feature vector embeddings instead.
      To better understand the process, we continue with the example given above.
      Lets assume that three new users register to the social network, nobody setting his / her workplace.
      Since some other mandatory information and maybe some links are given, the graph neural network can be used to predict the workplace of the new users nevertheless they haven't been included in the training process.
      This happens, by aggregating their neighborhood feature vectors with the own one and querying $f$ on the obtained vector.
      $f$ will then predict the workplace based on the provided aggregation, since it was trained to find patterns in vectors that lead to certain predictions.
      So, it is possible to train the graph neural network on some graph and apply it on others, since it is easy adjustable.
        
      Since the most real world problems like friendship prediction in social networks or protein-protein interactions in chemical networks cannot be modeled with a static graph, the inductive learning method is commonly used, while the transductive one isn't.
      In our work, we show that given an inductive trained graph neural network, that was trained on a graph $G_1$, we are able to extract information of another graph $G_2$ that is completely unknown to the model.
      More precisely, we steal links from $G_2$ using the prediction posteriors of the model while querying it on $G_2$.

  \section{Link Stealing Attacks}

    In recent work He et al. \cite{DBLP:journals/corr/abs-2005-02131} proposed the first link stealing attacks on graph neural networks.
    They focused on stealing links of the graph, that was used for training the given target model.
    Like described in Section 3.3.1 this is an attack on transductive trained graph neural networks.
    In our work, we want to show, that it is possible for an adversary to steal links from any graphs, given black-box access to an inductive trained target graph neural network model.
