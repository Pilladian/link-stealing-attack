\chapter{Background}

  \section{Neural Networks}

    Neural Networks (NNs) are key components in Artificial Intelligence (AI) and Deep Learning.
    They try to simulate some properties and the functionality of biological neural networks, like our brain, by imitating the way biological neural systems process data.
    Today, they have been applied successfully to speech recognition, face recognition on images or the transformation from speech to text.
    They are used to model software agents in video games, let autonomous robots learn new things or find patterns in data.

    A neural network consists of multiple layers.
    An input layer, one or many more hidden layers and an output layer.
    Each of these layers contain multiple neurons, which are represented as mathematical functions, that take multiple inputs and use them to calculate one output.
    Such neuron is also called perceptron, while fully connected neural networks are called multilayer perceptron (MLP).

    \vspace{0.48cm}
    \begin{figure}[!h]
      \begin{center}
        \begin{tikzpicture} [node distance = 2cm,
          on grid,
          %auto,
          %every state/.style = {draw = blue, fill = blue!30},
          every initial by arrow/.style = {font = \large,
              thick,-stealth
          }
          ]
          
          % Input Layer
          \node (i1) [state, initial, initial text= $input_1$] at (0,1) {};
          \node (i2) [state, initial, initial text= $input_2$] at (0,-1){};

          % Hidden Layer 1
          \node (h11) [state] at (2,2) {};
          \node (h12) [state] at (2,0) {};
          \node (h13) [state] at (2,-2) {};

          % Hidden Layer 2
          \node (h21) [state] at (4,2) {};
          \node (h22) [state] at (4,0) {};
          \node (h23) [state] at (4,-2) {};

          % Hidden Layer 2
          \node (h31) [state] at (6,2) {};
          \node (h32) [state] at (6,0) {};
          \node (h33) [state] at (6,-2) {};
          
          % Output Layer
          \node (o1) [state, accepting right, accepting text = {$output_1$}] at (8,1) {};
          \node (o2) [state, accepting right, accepting text = {$output_2$}] at (8,-1){};

          % Links
          \path [-stealth, thick]
            (i1) edge node {}   (h11)
            (i1) edge node {}   (h12)
            (i1) edge node {}   (h13)
            (i2) edge node {}   (h11)
            (i2) edge node {}   (h12)
            (i2) edge node {}   (h13)
            
            (h11) edge node {}   (h21)
            (h11) edge node {}   (h22)
            (h11) edge node {}   (h23)
            (h12) edge node {}   (h21)
            (h12) edge node {}   (h22)
            (h12) edge node {}   (h23)
            (h13) edge node {}   (h21)
            (h13) edge node {}   (h22)
            (h13) edge node {}   (h23)
            
            (h21) edge node {}   (h31)
            (h21) edge node {}   (h32)
            (h21) edge node {}   (h33)
            (h22) edge node {}   (h31)
            (h22) edge node {}   (h32)
            (h22) edge node {}   (h33)
            (h23) edge node {}   (h31)
            (h23) edge node {}   (h32)
            (h23) edge node {}   (h33)

            (h31) edge node {}   (o1)
            (h31) edge node {}   (o2)
            (h32) edge node {}   (o1)
            (h32) edge node {}   (o2)
            (h33) edge node {}   (o1)
            (h33) edge node {}   (o2);
        
        \end{tikzpicture}
      \end{center}

      \caption{Multilayer Perceptron}
      \label{figure:neural-network}
    \end{figure}

    \subsubsection*{Train a Neural Network}

      When we are confronted to a new task, we try to gain as much information as possible and based on them, we learn how to solve the problem.
      Neural networks behave similar.
      Before we can apply a neural network on classifying whether the object we provide is a spoon or fork, we need to tell the network, based on which information it should make its decision.
      That could be images of spoons and forks or their weight, length and width.
      We call this data \emph{Training Data}.
      To train the model in our example, we provide an image of a spoon and let the network make its decision.
      If the decision is correct, we wont change anything.
      But if the decision is incorrect, we need to slightly modify the weights - the connections between the perceptrons - to let the model behave different in the future.
      We do this for each image in our training set and repeat this process $n$ times (for $n$ epochs).
      After the training, we hopefully have a good trained neural network, which performs well on the provided task.

	\section{Graphs}

		As Graph we denote a data structure that contains nodes and edges. 
    Let $G = (V, E)$ be a graph with $V$ being the set of nodes and $E$ being the set of edges.
    We denote $\overrightarrow{a}$ as the feature vector of node $a$, representing the attributes of $a$.
    An edge $e = (i,j)$ contains the source node $i$ and the destination node $j$.
    In that way, links describe the relationship between the source and destination node. 
    The most popular example where graphs are used to model data, are social networks. 
    The nodes represent the users that have multiple attributes like location, gender, work place etc., while the edges state which relationship the users have.
    Lets consider a directed graph $G = (V,E)$ with $V$ representing the users and $E$ their relationships.
    If user $a$ follows user $b$, the edge $e_{ab} = (a,b)$ would be an element in $E$.
    If user $b$ follows user $a$, the edge $e_{ba} = (b,a)$ would be an element in $E$.
    Lets consider an undirected graph $G' = (V',E')$ with $V'$ representing the users and $E'$ their relationships.
    In $G'$ it doesn't matter whether user $a'$ follows $b'$ or the other way around. 
    Both results in $e'_{a'b'}, e'_{b'a'} \in E'$.
    Figure \refeq{figure:undirected-graph} shows an undirected graph.
    Since there is a link drawn between node $A$ and node $B$, we know, that there exists some relationship between them, but we don't know who follows whom ($e_{AB}, e_{BA} \in E$).
    The same holds for $e_{CF}, e_{FC} \in E$ or $e_{EG}, e_{GE} \in E$.
    Since there isn't a link drawn between node $E$ and node $C$, there doesn't exists a known relationship between them ($e_{EC}, e_{CE} \not\in E$).

    \vspace{0.48cm}
    \begin{figure}[!h]
      \begin{center}
        \begin{tikzpicture} [node distance = 2cm,
          on grid,
          %auto,
          %every state/.style = {draw = orange, fill = orange!30},
          every initial by arrow/.style = {font = \large,
              thick,-stealth
          }
          ]
          
          \node (a) [state] at (-1,2) {$A$};
          \node (b) [state] at (0,-1){$B$};
          \node (c) [state] at (3,1){$C$};
          \node (d) [state] at (5,-1){$D$};
          \node (e) [state] at (1,3){$E$};
          \node (f) [state] at (3,-2){$F$};
          \node (g) [state] at (7,3){$G$};

          % Links
          \draw (a) -- (b);
          \draw (d) -- (c);
          \draw (f) -- (a);
          \draw (a) -- (e);
          \draw (b) -- (f);
          \draw (c) -- (f);
          \draw (c) -- (g);
          \draw (d) -- (g);
          \draw (a) -- (c);
          \draw (e) -- (g);
        
        \end{tikzpicture}
      \end{center}

      \caption{Undirected Graph}
      \label{figure:undirected-graph}
    \end{figure}

	\section{Graph Neural Networks}
      
    % Describe how to query a graph neural network (f(Graph, node))
    In the last decades, social networks have become a huge part of life for many people all around the world.
    The companies behind these networks collect tons of data of each user every day.
    Let $G = (V, E)$ be a graph that models such a social network. 
    The set of all users is given as $V$ and $E$ represents their relationships.
    The feature vector $\overrightarrow{a}$ of user $a$ contains all information the company already has regarding this user.
    That could be name, relationship status, workplace, address, amount of children and so on.
    Lets consider one attribute (\emph{workplace}) as non mandatory for the registration.
    This leads to some users $v_{workplace} = \{v $ | $ \forall v \in V: v$ has $workplace$ given$\}$, the company knows the workplace from and some users $v_{unknown} = \{v $ | $ \forall v \in V: v$ has $workplace$ not given$\}$, where the attribute is unknown.
    Based on the information $v_{workplace}$ provides, a graph neural network model $f$ can be trained to predict the missing attribute of all users in $v_{unknown}$.
    This is a simple node classification task, where the label is the missing attribute $worplace$.
    There exist two major ideas of training a graph neural network - transductive or inductive.
    In our experiments introduced and described in Chapter \refeq{chapter:attacks} we focus on attacks on inductive trained graph neural networks. 

    \subsection*{Train a Graph Neural Network}

      To describe the process of training a graph neural network, we will stick to the example above.
      Let $f$ be the GNN we want to train on an undirected graph $G = (V,E)$ to perform some node classification task.
      We denote $workplace$ as label, that we want to predict and $V = v_{workplace} \cup v_{unknown}$ as set of nodes, containing every node, no matter whether the label is given or not.
      As our training dataset we define $d_{train} = \{(v, v_n)$ | $ \forall v \in v_{workplace}: v_n = neighborhood(v)\}$, where $neighborhood(n)$ is a function defined to collect the $k$-hop neighborhood of $n$ and aggregates their feature vectors. 
      $k = 0$ only considers $n$ and no neighbors of $n$.
      $k = 1$ also includes the neighbors of $n$, while $k = 2$ also aggregates the neighbors of the neighbors of $n$. 
      Depending on the type of GNN, this aggregation function $neighborhood()$ can differ in the algorithm collecting the neighborhood vectors.

      \subsubsection*{Transductive Learning}

        For transductive learning we consider all information as given, especially while training the graph neural network.
        More precisely, we denote, as mentioned before, $V = v_{workplace} \cup v_{unknown}$.
        This means, that for training the model on $d_{train}$, which contains all labeled nodes, the feature vectors of nodes, that are not labeled are included as well.
        Later when the model will be evaluated and tested, it already has seen the feature vectors of the nodes it should predict the labels of.
        So, the information of the nodes, the model needs to classify, is already known during the training phase.

      \subsubsection*{Inductive Learning}

        For inductive learning, we only consider the information of nodes that are labeled given while training the graph neural network.
        More precisely, we denote $V = v_{workplace}$ and $d_{train} = \{(v, v_n)$ | $ \forall v \in V: v_n = neighborhood(v)\}$.
        This means, that for training the model on $d_{train}$ only labeled nodes are used.
        Lets assume that three new users register to the social network, nobody setting his / her workplace.
        Since some other mandatory information and maybe some links are given, the graph neural network can be used to predict the workplace of the new users nevertheless they haven't been included in the training process.
        So, it is possible to train the graph neural network on some graph and apply it on others, since it is easy adjustable.
        
      Since the most real world problems like friendship prediction in social networks or protein-protein interactions in chemical networks cannot be modeled with a static graph, the inductive learning method is commonly used, while the transductive one isn't.
      In our work, we show that given an inductive trained graph neural network, that was trained on a graph $G_1$, we are able to extract information of another graph $G_2$ that is completely unknown to the model.
      More precisely, we steal links from $G_2$ using the prediction posteriors of the model while querying it on $G_2$.


