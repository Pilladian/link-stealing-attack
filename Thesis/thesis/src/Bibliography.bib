# Motivation
# ------------------------------------------------------------------------------
@article{DBLP:journals/corr/abs-2005-00687,
  author    = {Weihua Hu and
               Matthias Fey and
               Marinka Zitnik and
               Yuxiao Dong and
               Hongyu Ren and
               Bowen Liu and
               Michele Catasta and
               Jure Leskovec},
  title     = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
  journal   = {CoRR},
  volume    = {abs/2005.00687},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.00687},
  archivePrefix = {arXiv},
  eprint    = {2005.00687},
  timestamp = {Fri, 08 May 2020 15:04:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-00687.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{cook2006mining,
  title={Mining graph data},
  author={Cook, Diane J and Holder, Lawrence B},
  year={2006},
  publisher={John Wiley \& Sons}
}

@misc{atwood2016diffusionconvolutional,
      title={Diffusion-Convolutional Neural Networks},
      author={James Atwood and Don Towsley},
      year={2016},
      eprint={1511.02136},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{defferrard2017convolutional,
      title={Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
      author={Michaël Defferrard and Xavier Bresson and Pierre Vandergheynst},
      year={2017},
      eprint={1606.09375},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kipf2017semisupervised,
      title={Semi-Supervised Classification with Graph Convolutional Networks},
      author={Thomas N. Kipf and Max Welling},
      year={2017},
      eprint={1609.02907},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2018link,
      title={Link Prediction Based on Graph Neural Networks},
      author={Muhan Zhang and Yixin Chen},
      year={2018},
      eprint={1802.09691},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{5206871,
  author={Liu, Wei and Chang, Shih-Fu},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title={Robust multi-class transductive learning with graphs},
  year={2009},
  volume={},
  number={},
  pages={381-388},
  doi={10.1109/CVPR.2009.5206871}}

  @article{ZHA2010187,
  title = {Graph-based transductive learning for robust visual tracking},
  journal = {Pattern Recognition},
  volume = {43},
  number = {1},
  pages = {187-196},
  year = {2010},
  issn = {0031-3203},
  doi = {https://doi.org/10.1016/j.patcog.2009.06.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320309002581},
  author = {Yufei Zha and Yuan Yang and Duyan Bi},
  keywords = {Object tracking, Graph Laplacian, Transductive learning},
  abstract = {In object tracking problem, most methods assume brightness constancy or subspace constancy, which are violated in practice. In this paper, the object tracking problem is considered as a transductive learning problem and a robust tracking method is proposed under intrinsic and extrinsic varieties. The object not only fits the object model, but also has the same cluster with the previous objects, which are the labeled data. By constraining the global and local information, the cost function is constructed firstly. The solution for minimizing the cost function can be solved by a simple linear algebra with graph Laplacian. Moreover, a novel graph is constructed over the positive samples and candidate patches, which can simultaneously learn the object's global appearance model and the local intrinsic geometric structure of all the patches. Furthermore, a heuristic positive samples selection scheme is adopted to make the method more effective. The proposed method is tested on different videos, which undergo large pose, expression, illumination and partial occlusion, and compared with state-of-the-art algorithms. Experimental results and comparative studies are provided to demonstrate the efficiency of the proposed method.}
  }

  @article{WANG2017218,
title = {Multi-modal classification of neurodegenerative disease by progressive graph-based transductive learning},
journal = {Medical Image Analysis},
volume = {39},
pages = {218-230},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517300749},
author = {Zhengxia Wang and Xiaofeng Zhu and Ehsan Adeli and Yingying Zhu and Feiping Nie and Brent Munsell and Guorong Wu},
keywords = {Graph-based transductive learning (GTL), Multi-modality, Intrinsic representation, Computer-assisted diagnosis},
abstract = {Graph-based transductive learning (GTL) is a powerful machine learning technique that is used when sufficient training data is not available. In particular, conventional GTL approaches first construct a fixed inter-subject relation graph that is based on similarities in voxel intensity values in the feature domain, which can then be used to propagate the known phenotype data (i.e., clinical scores and labels) from the training data to the testing data in the label domain. However, this type of graph is exclusively learned in the feature domain, and primarily due to outliers in the observed features, may not be optimal for label propagation in the label domain. To address this limitation, a progressive GTL (pGTL) method is proposed that gradually finds an intrinsic data representation that more accurately aligns imaging features with the phenotype data. In general, optimal feature-to-phenotype alignment is achieved using an iterative approach that: (1) refines inter-subject relationships observed in the feature domain by using the learned intrinsic data representation in the label domain, (2) updates the intrinsic data representation from the refined inter-subject relationships, and (3) verifies the intrinsic data representation on the training data to guarantee an optimal classification when applied to testing data. Additionally, the iterative approach is extended to multi-modal imaging data to further improve pGTL classification accuracy. Using Alzheimer's disease and Parkinson's disease study data, the classification accuracy of the proposed pGTL method is compared to several state-of-the-art classification methods, and the results show pGTL can more accurately identify subjects, even at different progression stages, in these two study data sets.}
}

@InProceedings{10.1007/978-3-642-04174-7_29,
author="Talukdar, Partha Pratim
and Crammer, Koby",
editor="Buntine, Wray
and Grobelnik, Marko
and Mladeni{\'{c}}, Dunja
and Shawe-Taylor, John",
title="New Regularized Algorithms for Transductive Learning",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="442--457",
abstract="We propose a new graph-based label propagation algorithm for transductive learning. Each example is associated with a vertex in an undirected graph and a weighted edge between two vertices represents similarity between the two corresponding example. We build on Adsorption, a recently proposed algorithm and analyze its properties. We then state our learning algorithm as a convex optimization problem over multi-label assignments and derive an efficient algorithm to solve this problem. We state the conditions under which our algorithm is guaranteed to converge. We provide experimental evidence on various real-world datasets demonstrating the effectiveness of our algorithm over other algorithms for such problems. We also show that our algorithm can be extended to incorporate additional prior information, and demonstrate it with classifying data where the labels are not mutually exclusive.",
isbn="978-3-642-04174-7"
}

@misc{zeng2020graphsaint,
      title={GraphSAINT: Graph Sampling Based Inductive Learning Method},
      author={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},
      year={2020},
      eprint={1907.04931},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{8519335,
  author={Rossi, Ryan A. and Zhou, Rong and Ahmed, Nesreen K.},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  title={Deep Inductive Graph Representation Learning},
  year={2020},
  volume={32},
  number={3},
  pages={438-452},
  doi={10.1109/TKDE.2018.2878247}
}

@misc{zhang2020document,
      title={Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks},
      author={Yufeng Zhang and Xueli Yu and Zeyu Cui and Shu Wu and Zhongzhen Wen and Liang Wang},
      year={2020},
      eprint={2004.13826},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


# Related Work
# ------------------------------------------------------------------------------
@inproceedings{10.1145/1014052.1014066,
author = {Dalvi, Nilesh and Domingos, Pedro and Mausam and Sanghai, Sumit and Verma, Deepak},
title = {Adversarial Classification},
year = {2004},
isbn = {1581138881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1014052.1014066},
doi = {10.1145/1014052.1014066},
abstract = {Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations.},
booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {99–108},
numpages = {10},
keywords = {integer linear programming, naive Bayes, cost-sensitive learning, game theory, spam detection},
location = {Seattle, WA, USA},
series = {KDD '04}
}

@inproceedings{10.1145/1128817.1128824,
author = {Barreno, Marco and Nelson, Blaine and Sears, Russell and Joseph, Anthony D. and Tygar, J. D.},
title = {Can Machine Learning Be Secure?},
year = {2006},
isbn = {1595932720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1128817.1128824},
doi = {10.1145/1128817.1128824},
abstract = {Machine learning systems offer unparalled flexibility in dealing with evolving input in a variety of applications, such as intrusion detection systems and spam e-mail filtering. However, machine learning algorithms themselves can be a target of attack by a malicious adversary. This paper provides a framework for answering the question, "Can machine learning be secure?" Novel contributions of this paper include a taxonomy of different types of attacks on machine learning techniques and systems, a variety of defenses against those attacks, a discussion of ideas that are important to security for machine learning, an analytical model giving a lower bound on attacker's work function, and a list of open problems.},
booktitle = {Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security},
pages = {16–25},
numpages = {10},
keywords = {computer networks, game theory, security metrics, intrusion detection, spam filters, computer security, statistical learning, adversarial learning, machine learning},
location = {Taipei, Taiwan},
series = {ASIACCS '06}
}

@misc{szegedy2014intriguing,
      title={Intriguing properties of neural networks},
      author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
      year={2014},
      eprint={1312.6199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Biggio_2018,
   title={Wild patterns: Ten years after the rise of adversarial machine learning},
   volume={84},
   ISSN={0031-3203},
   url={http://dx.doi.org/10.1016/j.patcog.2018.07.023},
   DOI={10.1016/j.patcog.2018.07.023},
   journal={Pattern Recognition},
   publisher={Elsevier BV},
   author={Biggio, Battista and Roli, Fabio},
   year={2018},
   month={Dec},
   pages={317–331}
}

% Membership Inference
@misc{carlini2019secret,
      title={The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks},
      author={Nicholas Carlini and Chang Liu and Úlfar Erlingsson and Jernej Kos and Dawn Song},
      year={2019},
      eprint={1802.08232},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2018differentially,
      title={Differentially Private Data Generative Models},
      author={Qingrong Chen and Chong Xiang and Minhui Xue and Bo Li and Nikita Borisov and Dali Kaarfar and Haojin Zhu},
      year={2018},
      eprint={1812.02274},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{7958568,
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
  title={Membership Inference Attacks Against Machine Learning Models},
  year={2017},
  volume={},
  number={},
  pages={3-18},
  doi={10.1109/SP.2017.41}
}

@misc{truex2019demystifying,
      title={Towards Demystifying Membership Inference Attacks},
      author={Stacey Truex and Ling Liu and Mehmet Emre Gursoy and Lei Yu and Wenqi Wei},
      year={2019},
      eprint={1807.09173},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{hayes2018logan,
      title={LOGAN: Membership Inference Attacks Against Generative Models},
      author={Jamie Hayes and Luca Melis and George Danezis and Emiliano De Cristofaro},
      year={2018},
      eprint={1705.07663},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{10.1145/3319535.3363201,
author = {Jia, Jinyuan and Salem, Ahmed and Backes, Michael and Zhang, Yang and Gong, Neil Zhenqiang},
title = {MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363201},
doi = {10.1145/3319535.3363201},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {259–274},
numpages = {16},
keywords = {membership inference attacks, adversarial examples, privacy-preserving machine learning},
location = {London, United Kingdom},
series = {CCS '19}
}

@misc{salem2018mlleaks,
      title={ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models},
      author={Ahmed Salem and Yang Zhang and Mathias Humbert and Pascal Berrang and Mario Fritz and Michael Backes},
      year={2018},
      eprint={1806.01246},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{Li_2021,
   title={Membership Inference Attacks and Defenses in Classification Models},
   ISBN={9781450381437},
   url={http://dx.doi.org/10.1145/3422337.3447836},
   DOI={10.1145/3422337.3447836},
   journal={Proceedings of the Eleventh ACM Conference on Data and Application Security and Privacy},
   publisher={ACM},
   author={Li, Jiacheng and Li, Ninghui and Ribeiro, Bruno},
   year={2021},
   month={Apr}
}

% Model Inversion
@article {PMID:27077138,
	Title = {Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing},
	Author = {Fredrikson, Matthew and Lantz, Eric and Jha, Somesh and Lin, Simon and Page, David and Ristenpart, Thomas},
	Volume = {2014},
	Month = {August},
	Year = {2014},
	Journal = {Proceedings of the ... USENIX Security Symposium. UNIX Security Symposium},
	Pages = {17—32},
	Abstract = {We initiate the study of privacy in pharmacogenetics, wherein machine learning models are used to guide medical treatments based on a patient's genotype and background. Performing an in-depth case study on privacy in personalized warfarin dosing, we show that suggested models carry privacy risks, in particular because attackers can perform what we call &lt;i&gt;model inversion&lt;/i&gt;: an attacker, given the model and some demographic information about a patient, can predict the patient's genetic markers. As differential privacy (DP) is an oft-proposed solution for medical settings such as this, we evaluate its effectiveness for building private versions of pharmacogenetic models. We show that &lt;i&gt;DP mechanisms prevent our model inversion attacks when the privacy budget is carefully selected&lt;/i&gt;. We go on to analyze the impact on utility by performing simulated clinical trials with DP dosing models. We find that for privacy budgets effective at preventing attacks, &lt;i&gt;patients would be exposed to increased risk of stroke, bleeding events, and mortality&lt;/i&gt;. We conclude that &lt;i&gt;current&lt;/i&gt; DP mechanisms do not simultaneously improve genomic privacy while retaining desirable clinical efficacy, highlighting the need for new mechanisms that should be evaluated &lt;i&gt;in situ&lt;/i&gt; using the general methodology introduced by our work.},
	URL = {https://europepmc.org/articles/PMC4827719},
}

@inproceedings{10.1145/2810103.2813677,
  author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  title = {Model Inversion Attacks That Exploit Confidence Information and Basic Countermeasures},
  year = {2015},
  isbn = {9781450338325},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2810103.2813677},
  doi = {10.1145/2810103.2813677},
  abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.},
  booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
  pages = {1322–1333},
  numpages = {12},
  keywords = {machine learning, privacy, attacks},
  location = {Denver, Colorado, USA},
  series = {CCS '15}
}

@misc{chen2020improved,
  title={Improved Techniques for Model Inversion Attacks},
  author={Si Chen and Ruoxi Jia and Guo-Jun Qi},
  year={2020},
  eprint={2010.04092},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{8476925,
  author={Hidano, Seira and Murakami, Takao and Katsumata, Shuichi and Kiyomoto, Shinsaku and Hanaoka, Goichiro},
  booktitle={2017 15th Annual Conference on Privacy, Security and Trust (PST)},
  title={Model Inversion Attacks for Prediction Systems: Without Knowledge of Non-Sensitive Attributes},
  year={2017},
  volume={},
  number={},
  pages={115-11509},
  doi={10.1109/PST.2017.00023}}

% Model Extraction
@misc{atli2020extraction,
  title={Extraction of Complex DNN Models: Real Threat or Boogeyman?},
  author={Buse Gul Atli and Sebastian Szyller and Mika Juuti and Samuel Marchal and N. Asokan},
  year={2020},
  eprint={1910.05429},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{tramer2016stealing,
  title={Stealing Machine Learning Models via Prediction APIs},
  author={Florian Tramèr and Fan Zhang and Ari Juels and Michael K. Reiter and Thomas Ristenpart},
  year={2016},
  eprint={1609.02943},
  archivePrefix={arXiv},
  primaryClass={cs.CR}
}

@misc{8418595,
  author={Wang, Binghui and Gong, Neil Zhenqiang},
  booktitle={2018 IEEE Symposium on Security and Privacy (SP)},
  title={Stealing Hyperparameters in Machine Learning},
  year={2018},
  volume={},
  number={},
  pages={36-52},
  doi={10.1109/SP.2018.00038}
}

@misc{juuti2019prada,
  title={PRADA: Protecting against DNN Model Stealing Attacks},
  author={Mika Juuti and Sebastian Szyller and Samuel Marchal and N. Asokan},
  year={2019},
  eprint={1805.02628},
  archivePrefix={arXiv},
  primaryClass={cs.CR}
}

@misc{hu2021model,
  title={Model Extraction and Defenses on Generative Adversarial Networks},
  author={Hailong Hu and Jun Pang},
  year={2021},
  eprint={2101.02069},
  archivePrefix={arXiv},
  primaryClass={cs.CR}
}

@misc{goodfellow2014generative,
  title={Generative Adversarial Networks},
  author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  year={2014},
  eprint={1406.2661},
  archivePrefix={arXiv},
  primaryClass={stat.ML}
}

@inproceedings {272262,
  author = {Hengrui Jia and Christopher A. Choquette-Choo and Varun Chandrasekaran and Nicolas Papernot},
  title = {Entangled Watermarks as a Defense against Model Extraction},
  booktitle = {30th {USENIX} Security Symposium ({USENIX} Security 21)},
  year = {2021},
  url = {https://www.usenix.org/conference/usenixsecurity21/presentation/jia},
  publisher = {{USENIX} Association},
  month = aug,
}

@misc{mori2021bodame,
  title={BODAME: Bilevel Optimization for Defense Against Model Extraction},
  author={Yuto Mori and Atsushi Nitanda and Akiko Takeda},
  year={2021},
  eprint={2103.06797},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

% Graph Neural Networks
@misc{atwood2016diffusionconvolutional,
      title={Diffusion-Convolutional Neural Networks},
      author={James Atwood and Don Towsley},
      year={2016},
      eprint={1511.02136},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{4700287,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks},
  title={The Graph Neural Network Model},
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  doi={10.1109/TNN.2008.2005605}
}

@article{velickovic2018graph,
  title="{Graph Attention Networks}",
  author={Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  journal={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=rJXMpikCZ},
  note={accepted as poster},
}

@misc{agbcvmtgs,
  author = {Wang, Binghui and Gong, Neil},
  year = {2019},
  month = {03},
  pages = {},
  title = {Attacking Graph-based Classification via Manipulating the Graph Structure},
  doi = {10.13140/RG.2.2.29010.61124}
}

@article{Z_gner_2018,
  title={Adversarial Attacks on Neural Networks for Graph Data},
  ISBN={9781450355520},
  url={http://dx.doi.org/10.1145/3219819.3220078},
  DOI={10.1145/3219819.3220078},
  journal={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  publisher={ACM},
  author={Zügner, Daniel and Akbarnejad, Amir and Günnemann, Stephan},
  year={2018},
  month={Jul}
}

@misc{10.1145/3366423.3380149,
  author = {Sun, Yiwei and Wang, Suhang and Tang, Xianfeng and Hsieh, Tsung-Yu and Honavar, Vasant},
  title = {Adversarial Attacks on Graph Neural Networks via Node Injections: A Hierarchical Reinforcement Learning Approach},
  year = {2020},
  isbn = {9781450370233},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3366423.3380149},
  doi = {10.1145/3366423.3380149},
  abstract = {Graph Neural Networks (GNN) offer the powerful approach to node classification in complex networks across many domains including social media, E-commerce, and FinTech. However, recent studies show that GNNs are vulnerable to attacks aimed at adversely impacting their node classification performance. Existing studies of adversarial attacks on GNN focus primarily on manipulating the connectivity between existing nodes, a task that requires greater effort on the part of the attacker in real-world applications. In contrast, it is much more expedient on the part of the attacker to inject adversarial nodes, e.g., fake profiles with forged links, into existing graphs so as to reduce the performance of the GNN in classifying existing nodes. Hence, we consider a novel form of node injection poisoning attacks on graph data. We model the key steps of a node injection attack, e.g., establishing links between the injected adversarial nodes and other nodes, choosing the label of an injected node, etc. by a Markov Decision Process. We propose a novel reinforcement learning method for Node Injection Poisoning Attacks (NIPA), to sequentially modify the labels and links of the injected nodes, without changing the connectivity between existing nodes. Specifically, we introduce a hierarchical Q-learning network to manipulate the labels of the adversarial nodes and their links with other nodes in the graph, and design an appropriate reward function to guide the reinforcement learning agent to reduce the node classification performance of GNN. The results of the experiments show that NIPA is consistently more effective than the baseline node injection attack methods for poisoning graph data on three benchmark datasets. },
  booktitle = {Proceedings of The Web Conference 2020},
  pages = {673–683},
  numpages = {11},
  keywords = {Adversarial Attack, Reinforcement learning;, Graph Poisoning},
  location = {Taipei, Taiwan},
  series = {WWW '20}
}

@misc{jin2020adversarial,
      title={Adversarial Attacks and Defenses on Graphs: A Review, A Tool and Empirical Studies},
      author={Wei Jin and Yaxin Li and Han Xu and Yiqi Wang and Shuiwang Ji and Charu Aggarwal and Jiliang Tang},
      year={2020},
      eprint={2003.00653},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


























# Link Stealing Attacks
@article{DBLP:journals/corr/abs-2005-02131,
  author    = {Xinlei He and
               Jinyuan Jia and
               Michael Backes and
               Neil Zhenqiang Gong and
               Yang Zhang},
  title     = {Stealing Links from Graph Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2005.02131},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.02131},
  archivePrefix = {arXiv},
  eprint    = {2005.02131},
  timestamp = {Fri, 08 May 2020 15:04:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-02131.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
























@misc{simeone2018brief,
	title={A Brief Introduction to Machine Learning for Engineers},
	author={Osvaldo Simeone},
	year={2018},
	eprint={1709.02840},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{latouche2015graphs,
	title={Graphs in machine learning: an introduction},
	author={Pierre Latouche and Fabrice Rossi},
	year={2015},
	howpublished={\url{https://asdf.com}},
	eprint={1506.06962},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}
