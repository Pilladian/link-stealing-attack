\mbox{}
%\clearpage
\setstretch{1.3}  % Reset the line-spacing to 1.3 for body text (if it has changed)

% The Abstract Page
\addtotoc{Abstract}  % Add the "Abstract" page entry to the Contents
\abstract{

  % Since nowadays graphs are a common way to store and visualize data, Machine Learning algorithms have been improved to directly operate on them.
	% In most cases the graph itself can be deemed confidential, since the owner of the data often spends much time and resources collecting and preparing the data.
	% In our work, we show, that so called graph neural networks can reveal sensitive information about any graph they are queried on.
	% We focused on extracting information about the edges of the target graph by observing the predictions of the target model in so called link stealing attacks.
  % In prior work, He et al. proposed the first link stealing attacks on graph neural networks, focusing on the transductive learning.
  % More precisely, given a black box access to a graph neural network, they were able to predict, whether two nodes of a graph that was used for training, are linked or not.
  % In  our work, we now focus on the inductive setting.
  % Specifically, given a black box access to a graph neural network, we aim to predict whether there exists a link between any two nodes of any target graph, not only the one, the graph neural network was trained on.

  Since nowadays graphs are a common way to store and visualize data, Machine Learning algorithms have been improved to directly operate on them. 
  In most cases the graph itself can be deemed confidential, since the owner of the data often spends much time and resources collecting and preparing the data. 
  In our work, we show, that so called inductive trained graph neural networks can reveal sensitive information about their training graph.
  We focus on extracting information about the edges of the target graph by observing the predictions of the target model in so called link stealing attacks. 
  In prior work, He et al. proposed the first link stealing attacks on graph neural networks, focusing on the transductive learning setting. 
  More precisely, given a black box access to a graph neural network, they were able to predict, whether two nodes of a graph that was used for training the model, are linked or not. 
  In our work, we now focus on the inductive setting. 
  Specifically, given a black box access to a graph neural network model that was trained inductively, we aim to predict whether there exists a link between any two nodes of the training graph or not.
  \TODO{present results}

\addtocontents{toc}{\vspace{1em}}  % Add a gap in the Contents, for aesthetics

\clearpage  % Abstract ended, start a new page
